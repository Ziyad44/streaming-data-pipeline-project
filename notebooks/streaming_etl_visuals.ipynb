{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Streaming Data Pipeline (Kafka → Spark → Cassandra)\n",
    "\n",
    "This notebook walks through an end-to-end real-time data engineering pipeline:\n",
    "\n",
    "1. Generate mock user data from an API (Random User)\n",
    "2. Publish events to **Kafka** (topic: `users_created`)\n",
    "3. Process events with **Spark Structured Streaming**\n",
    "4. Store results in **Cassandra** (keyspace: `spark_streams`, table: `created_users`)\n",
    "5. Validate and visualize the stored data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project goal\n",
    "Build a small, reproducible streaming pipeline you can run locally with Docker.\n",
    "\n",
    "- **Ingestion:** Kafka\n",
    "- **Processing:** Spark Structured Streaming\n",
    "- **Storage:** Cassandra (with TTL to avoid unlimited growth)\n",
    "- **Orchestration / scheduling (optional):** Airflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System architecture\n",
    "\n",
    "`Producer (Python) → Kafka → Spark Streaming → Cassandra → Analytics / Visualization`\n",
    "\n",
    "Key components:\n",
    "- **Python Producer:** generates events and sends them to Kafka\n",
    "- **Kafka:** message broker / event log\n",
    "- **Spark:** reads Kafka stream, parses JSON, applies basic quality checks, writes to Cassandra\n",
    "- **Cassandra:** scalable NoSQL storage (table has **TTL** so old rows expire automatically)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Technology stack\n",
    "- **Orchestration:** Apache Airflow\n",
    "- **Streaming:** Apache Kafka + Confluent Platform\n",
    "- **Processing:** Apache Spark (Structured Streaming)\n",
    "- **Storage:** Cassandra\n",
    "- **Containerization:** Docker Compose\n",
    "- **Language:** Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- This notebook contains both **learning notes** and **pipeline code**.\n",
    "- Code cells use container hostnames inside Docker (e.g., `broker:29092`, `cassandra:9042`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Kafka primer (very short)\n",
    "- **Producer** publishes messages to a **topic**\n",
    "- **Consumer** reads messages from a topic\n",
    "- Messages are ordered per partition and tracked using **offsets**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka concepts\n",
    "- **Topic:** stream/category of events (like a channel)\n",
    "- **Partition:** parallelism & ordering unit\n",
    "- **Broker:** Kafka server\n",
    "- **Consumer group:** multiple consumers sharing the load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka data flow\n",
    "1. Producer sends events to a topic (`users_created`)\n",
    "2. Kafka stores events durably\n",
    "3. Spark consumes the topic continuously\n",
    "4. Spark writes processed rows to Cassandra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "Make sure Docker containers are running (`docker compose up -d`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Check versions inside containers (optional)\n",
    "The next cell prints Python and Spark versions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Check Python and Spark versions inside the Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version\n",
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Create mock streaming data from an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data from an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_data():\n",
    " # Fetch a random user\n",
    " res = requests.get(\"https://randomuser.me/api/\") # data \n",
    " res = res.json() # Convert JSON to dict\n",
    "    res = res['results'][0]\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(get_data(), indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the data as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def format_data(res):\n",
    " # Format the data\n",
    "    data = {}\n",
    "    location = res['location']\n",
    "    data['id'] = str(uuid.uuid4())\n",
    "    data['first_name'] = res['name']['first']\n",
    "    data['last_name'] = res['name']['last']\n",
    "    data['gender'] = res['gender']\n",
    "    data['address'] = f\"{str(location['street']['number'])} {location['street']['name']}, \" \\\n",
    "                      f\"{location['city']}, {location['state']}, {location['country']}\"\n",
    "    data['post_code'] = location['postcode']\n",
    "    data['email'] = res['email']\n",
    "    data['username'] = res['login']['username']\n",
    "    data['dob'] = res['dob']['date']\n",
    "    data['registered_date'] = res['registered']['date']\n",
    "    data['phone'] = res['phone']\n",
    "    data['picture'] = res['picture']['medium']\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data(get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock Api Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_data():\n",
    "    import json\n",
    "    from kafka import KafkaProducer\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    " producer = KafkaProducer(bootstrap_servers=['broker:29092'], max_block_ms=5000) # KafkaProducer Kafka Boker\n",
    "    curr_time = time.time() # real time\n",
    "\n",
    "    while True:\n",
    "        if time.time() > curr_time + 10: # 1 minute\n",
    "            break\n",
    "        try:\n",
    "            res = get_data()\n",
    "            res = format_data(res)\n",
    "\n",
    "            print(res)\n",
    " producer.send('users_created', json.dumps(res).encode('utf-8')) # res json boker\n",
    "        except Exception as e:\n",
    "            logging.error(f'An error occured: {e}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Streaming Data from API\n",
    "stream_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ETL (Extract, Transform, Load) Real-time** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (user) Kafka , , Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "def create_spark_connection():\n",
    "    s_conn = None\n",
    "\n",
    "    try:\n",
    "        s_conn = (SparkSession.builder \n",
    "            .appName('SparkDataStreaming')\n",
    "            .master(\"spark://spark-master:7077\")\n",
    " .config('spark.jars.packages', # Spark\n",
    " \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,\" # Load the Cassandra connector package\n",
    " \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") # Load the Kafka connector package \n",
    " .config('spark.cassandra.connection.host', 'localhost') # Cassandra\n",
    "            .getOrCreate())\n",
    "        \n",
    "        s_conn.sparkContext.setLogLevel(\"ERROR\")\n",
    "        logging.info(\"Spark connection created successfully!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Couldn't create the spark session due to exception {e}\")\n",
    "\n",
    "    return s_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka connection helper\n",
    "def connect_to_kafka(spark_conn):\n",
    "    spark_df = None\n",
    "    try:\n",
    "        spark_df = (spark_conn.readStream \\\n",
    "            .format('kafka') \\\n",
    " .option('kafka.bootstrap.servers', 'broker:29092') # Kafka Broker\n",
    " .option('subscribe', 'users_created') # Kafka Topic \n",
    " .option('startingOffsets', 'earliest') # earliest () latest ()\n",
    " .option(\"maxOffsetsPerTrigger\", 100) # 50 records\n",
    "            .load())\n",
    "        logging.info(\"kafka dataframe created successfully\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"kafka dataframe could not be created because: {e}\")\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra\n",
    "def create_cassandra_connection():\n",
    "    try:\n",
    "        # connecting to the cassandra cluster\n",
    " cluster = Cluster(contact_points=['cassandra'], port=9042) # hostname Cassandra\n",
    " cas_session = cluster.connect() # \n",
    "\n",
    "        logging.info(\"Connected to Cassandra!\")\n",
    "        return cas_session\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not create cassandra connection due to {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keyspace(session):\n",
    " # Database / Schema Cassandra Keyspace\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS spark_streams\n",
    "        WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Keyspace created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(session):\n",
    " # Create table\n",
    "    session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_streams.created_users (\n",
    "        id UUID PRIMARY KEY,\n",
    "        first_name TEXT,\n",
    "        last_name TEXT,\n",
    "        gender TEXT,\n",
    "        address TEXT,\n",
    "        post_code TEXT,\n",
    "        email TEXT,\n",
    "        username TEXT,\n",
    "        registered_date TEXT,\n",
    "        phone TEXT,\n",
    "        picture TEXT) WITH default_time_to_live = 1200;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Data Quality Checks\n",
    "def create_selection_df_from_kafka(spark_df):\n",
    "    \"\"\"\n",
    " Kafka Spark DataFrame Data Quality Checks\n",
    "    - Schema Enforcement\n",
    "    - Missing Value Check\n",
    "    - Business Rule Validation (regex, allowed values)\n",
    "    - Deduplication\n",
    " - validation Dead-letter topic (optional)\n",
    "    \n",
    "    Args:\n",
    " spark_df: input streaming DataFrame Kafka\n",
    " kafka_producer_invalid: KafkaProducer invalid (optional)\n",
    "    \n",
    "    Returns:\n",
    " validated_df: Spark DataFrame (validated)\n",
    "    \"\"\"\n",
    "    # -------------------- 1. Define Schema --------------------\n",
    " # Define the input schema\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"first_name\", StringType(), False),\n",
    "        StructField(\"last_name\", StringType(), False),\n",
    "        StructField(\"gender\", StringType(), False),\n",
    "        StructField(\"address\", StringType(), False),\n",
    "        StructField(\"post_code\", StringType(), False),\n",
    "        StructField(\"email\", StringType(), False),\n",
    "        StructField(\"username\", StringType(), False),\n",
    "        StructField(\"registered_date\", StringType(), False),\n",
    "        StructField(\"phone\", StringType(), False),\n",
    "        StructField(\"picture\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    # -------------------- 2. Parse JSON and Enforce Schema --------------------\n",
    " # Read and parse Kafka data into a Spark DataFrame \n",
    " df = (spark_df.selectExpr(\"CAST(value AS STRING)\") # value String \n",
    " .select(from_json(col('value'), schema).alias('data')).select(\"data.*\")) # json value schema\n",
    "    logging.info(\"Schema applied and JSON parsed.\")\n",
    "\n",
    "    # -------------------- 3. Missing Value Check --------------------\n",
    " df_non_null = df.dropna(subset=[\"id\", \"email\", \"registered_date\"]) # null col \n",
    "    logging.info(\"Dropped records with null id, email, or registered_date.\")\n",
    "\n",
    "    # -------------------- 4. Business Rule Validation --------------------\n",
    " df_valid = df_non_null.filter( # filter regex \n",
    " col(\"email\").rlike(r\".+@.+\\..+\") & # email pattern\n",
    " col(\"gender\").isin(\"male\", \"female\") & # 2 \n",
    "        col(\"id\").rlike(\"^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$\")\n",
    "    )\n",
    "    logging.info(\"Applied business rule validation.\")\n",
    "\n",
    "    # -------------------- 5. Deduplicate --------------------\n",
    " df_deduped = df_valid.dropDuplicates([\"id\"]) # id \n",
    "    logging.info(\"Deduplicated based on id.\")\n",
    "\n",
    "    return df_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark connection\n",
    "spark_conn = create_spark_connection() # Spark\n",
    "\n",
    "if spark_conn is not None:\n",
    "        # connect to kafka with spark connection\n",
    " spark_df = connect_to_kafka(spark_conn) # Kafka\n",
    " selection_df = create_selection_df_from_kafka(spark_df) # Kafka\n",
    " session = create_cassandra_connection() # Cassandra\n",
    "\n",
    "        # Testing Data Flow on Console\n",
    "        # query = (selection_df\n",
    "        #  .writeStream\n",
    "        #  .format(\"console\")\n",
    "        #  .outputMode(\"append\")\n",
    "        #  .start())\n",
    "\n",
    " # query.awaitTermination(30) # wait 30s query data\n",
    " # query.stop() # stop streaming query 30s\n",
    "\n",
    "        if session is not None:\n",
    " create_keyspace(session) # Database / Schema Cassandra Keyspace\n",
    " create_table(session) # Create table\n",
    "\n",
    "            logging.info(\"Streaming is being started...\")\n",
    "\n",
    " # Database\n",
    "            streaming_query = (selection_df.writeStream\n",
    "                               .format(\"org.apache.spark.sql.cassandra\")\n",
    "                               .option('checkpointLocation', '/tmp/checkpoint')\n",
    "                               .option('keyspace', 'spark_streams')\n",
    "                               .option('table', 'created_users')\n",
    "                               .option(\"spark.cassandra.connection.host\", \"cassandra\")\n",
    "                               .option(\"spark.cassandra.connection.port\", \"9042\")\n",
    "                               .option(\"spark.cassandra.connection.local_dc\", \"datacenter1\")\n",
    " .trigger(processingTime=\"10 seconds\") # 10 \n",
    "                               .start())\n",
    "\n",
    " streaming_query.awaitTermination(30) # query data\n",
    "            streaming_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (Cassandra → Pandas → Charts)\n",
    "\n",
    "The pipeline stores rows in Cassandra:\n",
    "- keyspace: `spark_streams`\n",
    "- table: `created_users`\n",
    "\n",
    "The table has **TTL** (default_time_to_live), so rows expire automatically after a while.\n",
    "\n",
    "Run the next cells to load recent rows into a DataFrame and visualize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# If you see missing packages, uncomment the next line:\n",
    "# !pip -q install cassandra-driver pandas matplotlib plotly\n",
    "\n",
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "def connect_cassandra():\n",
    "    \"\"\"Return a Cassandra session connected to keyspace spark_streams.\n",
    "\n",
    "    We try common hostnames used in Docker Compose:\n",
    "    - cassandra_db (service name)\n",
    "    - cassandra (container name / hostname)\n",
    "    - 127.0.0.1 (if you exposed port 9042 to the host)\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for host in [\"cassandra_db\", \"cassandra\", \"127.0.0.1\"]:\n",
    "        try:\n",
    "            cluster = Cluster([host], port=9042)\n",
    "            session = cluster.connect(\"spark_streams\")\n",
    "            print(f\"Connected to Cassandra via host: {host}\")\n",
    "            return session\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "\n",
    "    raise RuntimeError(f\"Could not connect to Cassandra on any host. Last error: {last_err}\")\n",
    "\n",
    "session = connect_cassandra()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load up to N rows from Cassandra\n",
    "N = 2000\n",
    "rows = session.execute(f\"SELECT * FROM created_users LIMIT {N}\")\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Rows loaded:\", len(df))\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic data quality checks\n",
    "\n",
    "# 1) Missing values per column\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parse the registered_date column (stored as TEXT) if present\n",
    "# This enables time-based charts.\n",
    "\n",
    "if \"registered_date\" in df.columns:\n",
    "    df[\"registered_date\"] = pd.to_datetime(df[\"registered_date\"], errors=\"coerce\")\n",
    "\n",
    "(df.dtypes)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chart 1: gender distribution (if present)\n",
    "if \"gender\" in df.columns:\n",
    "    ax = df[\"gender\"].value_counts().plot(kind=\"bar\")\n",
    "    ax.set_title(\"Gender distribution (created_users)\")\n",
    "    ax.set_xlabel(\"gender\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No 'gender' column found.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Chart 2: users created per minute (if registered_date parsed)\n",
    "if \"registered_date\" in df.columns and df[\"registered_date\"].notna().any():\n",
    "    tmp = df.dropna(subset=[\"registered_date\"]).set_index(\"registered_date\")\n",
    "    ax = tmp.resample(\"1min\").size().plot()\n",
    "    ax.set_title(\"Users created per minute\")\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.set_ylabel(\"users\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"registered_date not available or not parseable.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Chart 3: top email domains (quick, useful)\n",
    "if \"email\" in df.columns:\n",
    "    domains = df[\"email\"].astype(str).str.split(\"@\").str[-1]\n",
    "    ax = domains.value_counts().head(10).plot(kind=\"bar\")\n",
    "    ax.set_title(\"Top 10 email domains\")\n",
    "    ax.set_xlabel(\"domain\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No 'email' column found.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: interactive charts (Plotly)\n",
    "If you want nicer interactive visuals, run the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Optional interactive chart\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    if \"gender\" in df.columns:\n",
    "        fig = px.histogram(df, x=\"gender\", title=\"Gender distribution (interactive)\")\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"No 'gender' column found.\")\n",
    "except Exception as e:\n",
    "    print(\"Plotly not available or failed to render:\", e)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}