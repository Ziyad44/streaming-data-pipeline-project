{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480b2e1",
   "metadata": {},
   "source": [
    "# Real-time Streaming Playbook\n",
    "\n",
    "Practical tips to keep a Kafka ‚Üí Spark ‚Üí Cassandra pipeline stable on a laptop.\n",
    "\n",
    "Use this as an operational checklist when your message rate increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ff4d7",
   "metadata": {},
   "source": [
    "## 1) Control the input rate (Producer / Kafka)\n",
    "- If the producer sends too fast, queues grow and everything downstream gets overloaded.\n",
    "- Options:\n",
    "  - **Throttle the producer** (sleep / rate-limit)\n",
    "  - **Batch events** before sending to Kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd4d87",
   "metadata": {},
   "source": [
    "## 2) Control the Spark streaming rate\n",
    "Spark runs in micro-batches.\n",
    "\n",
    "Useful knobs:\n",
    "- `trigger(processingTime=\"10 seconds\")` to slow down batch frequency\n",
    "- `maxOffsetsPerTrigger` to limit how many Kafka messages Spark reads per batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d65a56",
   "metadata": {},
   "source": [
    "## 3) Use TTL / upserts in Cassandra\n",
    "- If you do not need to keep all records forever, use **TTL** to prevent unlimited disk usage.\n",
    "- If you need deduplication, use a stable primary key and write in an idempotent way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e93e6a3",
   "metadata": {},
   "source": [
    "## 4) Checkpointing & restart safety\n",
    "Always set a `checkpointLocation`.\n",
    "\n",
    "- Prevents duplicate processing on restart\n",
    "- Allows Spark to resume from the last committed offsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c46fc",
   "metadata": {},
   "source": [
    "```python\n# Cassandra connection (write stream)\nstreaming_query = (selection_df.writeStream\n                   .format(\"org.apache.spark.sql.cassandra\")\n                   .option(\"checkpointLocation\", \"/tmp/checkpoint\")\n                   .option(\"keyspace\", \"spark_streams\")\n                   .option(\"table\", \"created_users\")\n                   .option(\"spark.cassandra.connection.host\", \"cassandra\")\n                   .option(\"spark.cassandra.connection.port\", \"9042\")\n                   .option(\"spark.cassandra.connection.local_dc\", \"datacenter1\")\n .trigger(processingTime=\"10 seconds\") # \n                   .start())\n```\n\nNote: `selection_df` and `spark_conn` refer to the DataFrames/session created in the main ETL notebook. This playbook keeps the code as a reference snippet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d32fb5",
   "metadata": {},
   "source": [
    "- `processingTime=\"10 seconds\"` ‚Üí Spark will create a micro-batch every 10 \n\n- batch Kafka batch \n\n- This reduces write requests per second ‚Üí Cassandra / Database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f1c33",
   "metadata": {},
   "source": [
    "- 2.Backpressure / Rate \n\n - offset batch (optional, batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7a913",
   "metadata": {},
   "source": [
    "```python\n# Read streaming data from Kafka\nspark_df = (spark_conn.readStream\n            .format(\"kafka\")\n            .option(\"kafka.bootstrap.servers\", \"broker:29092\")\n            .option(\"subscribe\", \"users_created\")\n            .option(\"startingOffsets\", \"earliest\")\n .option(\"maxOffsetsPerTrigger\", 1000) # 1000 records\n            .load())\n```\n\nNote: `selection_df` and `spark_conn` refer to the DataFrames/session created in the main ETL notebook. This playbook keeps the code as a reference snippet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5ab1a",
   "metadata": {},
   "source": [
    "- data Spark ‚Üí batch 1000 record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd203685",
   "metadata": {},
   "source": [
    "##### 3Ô∏è‚É£ Upsert / TTL Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474c3f3",
   "metadata": {},
   "source": [
    "- record ‚Üí TTL (Time-to-Live)\n\n- upsert insert (key conflicts)\n\n- data accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e4ff2",
   "metadata": {},
   "source": [
    "##### 4Ô∏è‚É£ Checkpoint & Restart Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc1716",
   "metadata": {},
   "source": [
    "- `checkpointLocation` Spark batch \n\n- Realtime Streaming ‚Üí DB\n\n- Spark /Restart "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760045c",
   "metadata": {},
   "source": [
    "##### 5Ô∏è‚É£ Partitioning / Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574af87",
   "metadata": {},
   "source": [
    "- Cassandra write partition\n\n- Spark partition match Cassandra partition key ‚Üí hotspot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68497141",
   "metadata": {},
   "source": [
    "##### üí° :\n\n- stream ‚Äú record‚Äù \n\n- trigger interval + maxOffsetsPerTrigger\n\n- checkpoint + TTL Cassandra\n\n- schema partition high throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746ce41",
   "metadata": {},
   "source": [
    "### **Data Quality Checks (DQC) Data Validation**\n\n- Realtime ETL transform EDA + data quality Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbf5b5",
   "metadata": {},
   "source": [
    "##### üîπ Batch ETL + EDA ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25943f99",
   "metadata": {},
   "source": [
    "- \n\n- EDA missing, distribution, outlier, correlation\n\n- dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8dbbd",
   "metadata": {},
   "source": [
    "##### üîπ Realtime ETL + Transform "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ced8a",
   "metadata": {},
   "source": [
    "- EDA data stream\n\n- Data Quality Checks (DQC) Data Validation real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5438a",
   "metadata": {},
   "source": [
    "### ** API Streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4852444",
   "metadata": {},
   "source": [
    "#### üîπ API Streaming 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee65ae5e",
   "metadata": {},
   "source": [
    "**1. Long-lived HTTP connection (HTTP Streaming / SSE ‚Äì Server-Sent Events)**\n\n- request ‚Üí connection ‚Üí server push \n\n- Twitter Streaming API (), Stock market feed, IoT sensor data\n\n- Python/Java connection loop record record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7639772",
   "metadata": {},
   "source": [
    "**2. WebSocket API**\n\n- 2-way connection ‚Üí client/server \n\n- real-time Chat, Crypto price feed, Game server\n\n- library `websockets` (Python), `socket.io`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea09bd",
   "metadata": {},
   "source": [
    "**3. Polling API (simulate streaming)**\n\n- streaming ‚Üí client call REST API ( 1 /10 )\n\n- ‚Äústream‚Äù micro-batch\n\n- Airflow Cron jobs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92bdb4e",
   "metadata": {},
   "source": [
    "#### üîπ \n\n- **REST API ()** ‚Üí \n - ‚Üí real-time polling ( ) 1 , 5 \n - ‚Üí Airflow Airflow = scheduler batch/micro-batch\n\n- **Streaming API ( WebSocket, gRPC stream, SSE, MQTT)** ‚Üí connect push sensor feed \n - ‚Üí Airflow run loop kafka \n - ‚Üí Spark Structured Streaming, Flink, Kafka Consumer custom daemon script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885b1c3",
   "metadata": {},
   "source": [
    "##### ‚úÖ Pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a3c43",
   "metadata": {},
   "source": [
    "**1. REST API + Airflow**\n\n- Airflow operator 1 \n\n- storage ( Cassandra, BigQuery, S3, Postgres)\n\n- micro-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea28f6",
   "metadata": {},
   "source": [
    "**2. Streaming API + Spark/Kafka**\n\n- Spark structured streaming WebSocket ‚Üí push Kafka ‚Üí consumer DB\n\n- real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1caa0d",
   "metadata": {},
   "source": [
    "**3. Hybrid**\n\n- API streaming ‚Üí polling (Airflow Spark Structured Streaming + foreachBatch())\n\n- API streaming ‚Üí Spark/Kafka, schedule downstream Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50d4b3",
   "metadata": {},
   "source": [
    "## 5) Monitoring (what to look at)\n",
    "- **Kafka Control Center:** topic throughput, lag, consumer group health\n",
    "- **Airflow:** DAG runs are green and stable\n",
    "- **Cassandra:** row counts and disk usage (TTL should keep data bounded)\n",
    "\n",
    "Quick Cassandra check (from terminal):\n",
    "```bash\n",
    "docker compose exec cassandra_db cqlsh\n",
    "```\n",
    "```sql\n",
    "USE spark_streams;\n",
    "SELECT COUNT(*) FROM created_users;\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}